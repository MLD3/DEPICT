{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the noteboook auto reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T13:48:37.516879Z",
     "iopub.status.busy": "2024-08-28T13:48:37.516410Z",
     "iopub.status.idle": "2024-08-28T13:48:37.535676Z",
     "shell.execute_reply": "2024-08-28T13:48:37.535009Z",
     "shell.execute_reply.started": "2024-08-28T13:48:37.516843Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPICT COCO Preprocessing Instructions:\n",
    "- Run each cell sequentially in order to produce the `DEPICT_coco.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: None! All downloads will be handeled by the notebook internally.  \n",
    "- Outputs: `DEPICT_coco.csv` (the base dataframe used to replicate DEPICT results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Necessary Coco Files (uncomment upon first run of cell!)\n",
    "- This cell will create two new folders: images/ and annotations/\n",
    "- `images/` will hold the downloaded coco images\n",
    "- `annocations/` will hold the concept labels from coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-20 13:14:57--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.97.4, 54.231.162.217, 54.231.132.65, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.97.4|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘annotations_trainval2017.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.19M  2.36MB/s    in 65s     \n",
      "\n",
      "2024-08-20 13:16:02 (3.71 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Archive:  annotations_trainval2017.zip\n",
      "  inflating: annotations/instances_train2017.json  \n",
      "  inflating: annotations/instances_val2017.json  \n",
      "  inflating: annotations/captions_train2017.json  \n",
      "  inflating: annotations/captions_val2017.json  \n",
      "  inflating: annotations/person_keypoints_train2017.json  \n",
      "  inflating: annotations/person_keypoints_val2017.json  \n"
     ]
    }
   ],
   "source": [
    "# !chmod +x get_coco.sh\n",
    "# !./get_coco.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "random.seed(88)\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cell below will extract information from the coco_train2017.json and coco_val2017.json files into more easily usable datastructures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 543831.96it/s]\n",
      "100%|██████████| 896782/896782 [00:00<00:00, 1534458.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Builds 'cat_dict': key category_id | val: name\n",
    "#category_id corresponds to a concept's object id in the origional coco dataset\n",
    "train_file = open(os.path.join('annotations','instances_train2017.json'))\n",
    "train_data = json.load(train_file)\n",
    "\n",
    "val_file = open(os.path.join('annotations','instances_val2017.json'))\n",
    "val_data = json.load(val_file)\n",
    "\n",
    "annotations_allInfo = train_data['annotations'] + val_data['annotations']\n",
    "categories = train_data['categories']\n",
    "\n",
    "# 'images' (list) will store filename, id, and split for each image used later\n",
    "images =[]\n",
    "\n",
    "for img in train_data['images']:\n",
    "    images.append({'file_name': img['file_name'],\n",
    "                   'id': img['id'],\n",
    "                  'split':'train'})\n",
    "\n",
    "for img in val_data['images']:\n",
    "    images.append({'file_name': img['file_name'],\n",
    "                   'id': img['id'],\n",
    "                  'split':'val'})\n",
    "\n",
    "#cat_dict: key category_id | val: name\n",
    "cat_dict = {}\n",
    "for obj in tqdm(categories):\n",
    "    if obj['id'] not in cat_dict:\n",
    "        cat_dict[obj['id']] = obj['name']\n",
    "\n",
    "#Builds dictionary mapping image_id --> concepts in image\n",
    "#annotations: key: image_id | val:[category_id,...,category_id]\n",
    "annotations = {}\n",
    "for ann in tqdm(annotations_allInfo):\n",
    "    image_id = ann['image_id']\n",
    "    category_id = ann['category_id']\n",
    "    if image_id not in annotations:\n",
    "        annotations[image_id] = [category_id]\n",
    "    else:\n",
    "        annotations[image_id].append(category_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build DataFrame of Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123287 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:03<00:00, 31193.00it/s]\n"
     ]
    }
   ],
   "source": [
    "concept_list = list(cat_dict.values())\n",
    "feature_to_init_concept_idx = {name:i for i,name in enumerate(cat_dict.values())}\n",
    "toDf = []\n",
    "\n",
    "# Loop through each image and build its row in the dataframe including the image_id, filename, split, and concept counts\n",
    "for i, img in enumerate(tqdm(images)):\n",
    "    id = img['id']\n",
    "    file_name = os.getcwd()+ f\"/images/{img['split']}2017/\" + img['file_name']\n",
    "    if id in annotations:\n",
    "        category_ids = annotations[id]\n",
    "        unique_ids, counts = np.unique(category_ids, return_counts=True)\n",
    "        unique_names = [cat_dict[cat_id] for cat_id in unique_ids]\n",
    "        init_concepts = np.zeros(80,dtype=np.int8).tolist()\n",
    "        caption = \"\"\n",
    "        for count, name in zip(counts, unique_names):\n",
    "            caption += f\"{count} {name}, \"\n",
    "            init_concepts[feature_to_init_concept_idx[name]] = count\n",
    "            \n",
    "        #remove unneeded final comma and space from caption\n",
    "        caption = caption[:-2]\n",
    "        #Default all coco_train2017 images to train, will set test split in cells below\n",
    "        toDf.append([id, file_name, caption, *init_concepts, img['split']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Created Dataframe has correct number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: The df we are building doesn't yet have the scene label columns present in df_final.\n",
      "Current number of rows: 122218\n",
      "Yay! The df we are building has the correct number of rows!\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(toDf,columns=[\"image_id\",\"file_name\",\"text\",*concept_list,\"split\"])\n",
    "print(\"Note: The df we are building doesn't yet have the scene label columns present in df_final.\") \n",
    "print(f\"Current number of rows:\",len(df))\n",
    "assert(122218 == len(df))\n",
    "print(\"Yay! The df we are building has the correct number of rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify current train split size (soon to be split into train/test). Expect 117,266 images currently in train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_train_split_size = len(df.loc[df['split']=='train'])\n",
    "print(cur_train_split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Final Train/Test Split: currently train/test images are all marked as \"train\" in their split column. Among these images, we will randomly select 10k to be in the test set with a seed set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay! The train/val/test image id splits match the official DEPICT splits.\n",
      "split\n",
      "train    107266\n",
      "test      10000\n",
      "val        4952\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "random.seed(88)\n",
    "# Sample 10k index id's among all image_id's (117,266 image_id's in total)\n",
    "# +1 as right bound is exclusive\n",
    "test_idxs = random.sample(range(0,cur_train_split_size+1),10000)\n",
    "# Label corresponding idxs split as test\n",
    "df.loc[test_idxs,'split'] = 'test'\n",
    "# Get split counts\n",
    "counts = df['split'].value_counts()\n",
    "\n",
    "#load in official coco train/val/test image ids for comparison\n",
    "df_DEPICT_train_ids = pd.read_csv(\"DEPICT_splits/DEPICT_coco_train_img_ids.csv\")\n",
    "df_DEPICT_val_ids = pd.read_csv(\"DEPICT_splits/DEPICT_coco_val_img_ids.csv\")\n",
    "df_DEPICT_test_ids = pd.read_csv(\"DEPICT_splits/DEPICT_coco_test_img_ids.csv\")\n",
    "\n",
    "#Verify the train/val/test image ids generated by notebook match official DEPICT splits\n",
    "assert(set((df.loc[df['split']=='train'].image_id)) == set(df_DEPICT_train_ids.id))\n",
    "assert(set((df.loc[df['split']=='val'].image_id)) == set(df_DEPICT_val_ids.id))\n",
    "assert(set((df.loc[df['split']=='test'].image_id)) == set(df_DEPICT_test_ids.id))\n",
    "print(\"Yay! The train/val/test image id splits match the official DEPICT splits.\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Coco Concepts DataFrame Intermediate for Scene Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DEPICT_coco_concepts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Device as apporiate for your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available()  else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Scene Ids from MIT Places365 & Format Scene Id DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-20 15:30:34--  https://raw.githubusercontent.com/CSAILVision/places365/master/categories_places365.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6833 (6.7K) [text/plain]\n",
      "Saving to: ‘categories_places365.txt’\n",
      "\n",
      "categories_places36 100%[===================>]   6.67K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-08-20 15:30:34 (55.8 MB/s) - ‘categories_places365.txt’ saved [6833/6833]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://raw.githubusercontent.com/CSAILVision/places365/master/categories_places365.txt\n",
    "\n",
    "df_sceneIdxs = pd.read_csv(\"categories_places365.txt\",header=None,)\n",
    "df_sceneIdxs.columns = [\"category\"]\n",
    "df_sceneIdxs[\"category\"] = df_sceneIdxs[\"category\"].apply(lambda x:x.split()[0])\n",
    "df_sceneIdxs.to_csv(\"sceneIdxs.csv\",index=False)\n",
    "!rm -rf categories_places365.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Scene Labels for Coco Images Using MIT Places365 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataset\n",
    "import sys\n",
    "\n",
    "\n",
    "parent_output = \"scene_info\"\n",
    "os.makedirs(parent_output,exist_ok=True)\n",
    "# ############################################################\n",
    "# code used from: https://github.com/CSAILVision/places365/blob/master/run_placesCNN_basic.py\n",
    "# boxed code is from that page\n",
    "############################################################\n",
    "# th architecture to use (they have lots of other ones to try on that link above??)\n",
    "arch = 'resnet18'\n",
    "\n",
    "#load the pre-trained weights\n",
    "model_file = '%s_places365.pth.tar' % arch\n",
    "if not os.access(model_file, os.W_OK):\n",
    "    weight_url = 'http://places2.csail.mit.edu/models_places365/' + model_file\n",
    "    os.system('wget ' + weight_url)\n",
    "\n",
    "\n",
    "model = models.__dict__[arch](num_classes=365)\n",
    "checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "################################################################################\n",
    "\n",
    "names = []\n",
    "for i in range(1,366):\n",
    "    names.append(str(i))\n",
    "\n",
    "pred_out = []\n",
    "certainty_out = []\n",
    "\n",
    "#index corresponds with model output index\n",
    "#Ex) scene_idxs[0] = airfield\n",
    "scene_idxs = pd.read_csv('sceneIdxs.csv')['category'].to_numpy()\n",
    "\n",
    "print(\"generating labels\")\n",
    "# loader uses the image transformations that used by MIT Places model\n",
    "loader = dataset.get_loader(128)\n",
    "with torch.no_grad():\n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    j = 1\n",
    "    for ids, imgs in tqdm(loader):\n",
    "        print(f\"starting itteration: {j}\")\n",
    "        j +=1\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "\n",
    "        #idxs sorted high --> low predictions\n",
    "        preds = preds.cpu().numpy()\n",
    "        sorted_idxs = np.argsort(preds, axis=1)\n",
    "        sorted_idxs = np.fliplr(sorted_idxs)\n",
    "        for i,single_sort in enumerate(sorted_idxs):\n",
    "            scene_rankings = scene_idxs[single_sort]\n",
    "            pred_out.append([ids[i].item(),*scene_rankings])\n",
    "            row_preds = softmax(torch.tensor(preds[i][single_sort]))\n",
    "            certainty_out.append([ids[i].item(),*(row_preds.tolist())])\n",
    "\n",
    "pred_out_df = pd.DataFrame(data=pred_out,columns=['image_id',*names])\n",
    "certainty_out_df = pd.DataFrame(data=certainty_out,columns=['image_id',*names])\n",
    "\n",
    "# Save MIT Places CNN Scene predictions\n",
    "pred_out_df.to_csv(f'{parent_output}/id_to_scene.csv',index=False)\n",
    "\n",
    "# Save Prediction certainty for each image's predicted scene\n",
    "certainty_out_df.to_csv(f'{parent_output}/id_to_pred.csv', index=False)\n",
    "\n",
    "print(\"saved to: id_to_scene.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the MIT Places Scene Hierarchy information as csv\n",
    "- View the official MIT Places365 Scene Hierarchy Spreadsheet [HERE](https://docs.google.com/spreadsheets/d/1H7ADoEIGgbF_eXh9kcJjCs5j_r3VJwke4nebhkdzksg/edit?gid=142478777#gid=142478777)\n",
    "- For DEPICT, we are interested in the Level 2 Indoor Scenes Consisting of: \n",
    "    1) shopping and dining\n",
    "    2) workplace (office building, factory, lab, etc.)\n",
    "    3) home or hotel\n",
    "    4) transportation (vehicle interiors, stations, etc.)\n",
    "    5) sports and leisure\n",
    "    6) cultural (art, education, religion, millitary, law, politics, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have sceneHierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"sceneHierarchy.csv\"):\n",
    "    !wget --output-document=sceneHierarchy.csv \"https://docs.google.com/spreadsheets/d/1H7ADoEIGgbF_eXh9kcJjCs5j_r3VJwke4nebhkdzksg/export?format=csv&gid=142478777\"\n",
    "else:\n",
    "    print(\"already have sceneHierarchy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below:\n",
    "1) formats the sceneHierarch.csv\n",
    "2) reads in the scene predictions generated by MIT Places365 Resnet\n",
    "3) joins scene predictions with corresponding scene hierarchy for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_df = pd.read_csv('sceneHierarchy.csv')\n",
    "mother_df = mother_df.rename({'Unnamed: 0':\"scene\"},axis=1)\n",
    "mother_df.columns = mother_df.iloc[0]\n",
    "mother_df.drop(0,axis=0,inplace=True)\n",
    "mother_df['category'] = mother_df['category'].apply(lambda x:x.split(\"'\")[1])\n",
    "mother_df.set_index('category',inplace=True)\n",
    "\n",
    "# NOTE: 'parent_output' is set in the previous cell\n",
    "id_to_sc_df = pd.read_csv(f'{parent_output}/id_to_scene.csv')\n",
    "id_to_sc_df = id_to_sc_df[['image_id','1']]\n",
    "id_to_sc_df = id_to_sc_df.rename({'1':'category'},axis=1)\n",
    "id_to_sc_df.set_index('category',inplace=True)\n",
    "\n",
    "id_to_sceneInfo_df = id_to_sc_df.join(mother_df)\n",
    "id_to_sceneInfo_df.reset_index(inplace=True)\n",
    "id_to_sceneInfo_df['image_id'] = id_to_sceneInfo_df['image_id'].astype(int)\n",
    "id_to_sceneInfo_df = id_to_sceneInfo_df[['image_id', 'category'] + id_to_sceneInfo_df.columns[2:].tolist()]\n",
    "#Set binary columns to int\n",
    "for col in id_to_sceneInfo_df.columns[2:]:\n",
    "    id_to_sceneInfo_df[col] = id_to_sceneInfo_df[col].astype(int)\n",
    "id_to_sceneInfo_df.to_csv(f'{parent_output}/id_to_sceneInfo.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge scene and coco concept info a DataFrame and save final DataFrame as `coco_DEPICT.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_merged shape: (122218, 104)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>person</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>car</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>airplane</th>\n",
       "      <th>bus</th>\n",
       "      <th>train</th>\n",
       "      <th>...</th>\n",
       "      <th>water, ice, snow</th>\n",
       "      <th>mountains, hills, desert, sky</th>\n",
       "      <th>forest, field, jungle</th>\n",
       "      <th>man-made elements</th>\n",
       "      <th>transportation (roads, parking, bridges, boats, airports, etc.)</th>\n",
       "      <th>cultural or historical building/place (millitary, religious)</th>\n",
       "      <th>sports fields, parks, leisure spaces</th>\n",
       "      <th>industrial and construction</th>\n",
       "      <th>houses, cabins, gardens, and farms</th>\n",
       "      <th>commercial buildings, shops, markets, cities, and towns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391895</td>\n",
       "      <td>/data2/diffusion/notebook/coco_preprocessing/i...</td>\n",
       "      <td>2 person, 1 bicycle, 1 motorcycle</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>522418</td>\n",
       "      <td>/data2/diffusion/notebook/coco_preprocessing/i...</td>\n",
       "      <td>1 person, 1 knife, 1 cake, 1 sink</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184613</td>\n",
       "      <td>/data2/diffusion/notebook/coco_preprocessing/i...</td>\n",
       "      <td>14 person, 9 cow, 1 umbrella</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318219</td>\n",
       "      <td>/data2/diffusion/notebook/coco_preprocessing/i...</td>\n",
       "      <td>2 person, 3 tv, 4 mouse, 2 keyboard</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>554625</td>\n",
       "      <td>/data2/diffusion/notebook/coco_preprocessing/i...</td>\n",
       "      <td>5 person, 5 tv, 5 mouse, 4 keyboard</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                          file_name  \\\n",
       "0    391895  /data2/diffusion/notebook/coco_preprocessing/i...   \n",
       "1    522418  /data2/diffusion/notebook/coco_preprocessing/i...   \n",
       "2    184613  /data2/diffusion/notebook/coco_preprocessing/i...   \n",
       "3    318219  /data2/diffusion/notebook/coco_preprocessing/i...   \n",
       "4    554625  /data2/diffusion/notebook/coco_preprocessing/i...   \n",
       "\n",
       "                                  text  person  bicycle  car  motorcycle  \\\n",
       "0    2 person, 1 bicycle, 1 motorcycle       2        1    0           1   \n",
       "1    1 person, 1 knife, 1 cake, 1 sink       1        0    0           0   \n",
       "2         14 person, 9 cow, 1 umbrella      14        0    0           0   \n",
       "3  2 person, 3 tv, 4 mouse, 2 keyboard       2        0    0           0   \n",
       "4  5 person, 5 tv, 5 mouse, 4 keyboard       5        0    0           0   \n",
       "\n",
       "   airplane  bus  train  ...  water, ice, snow  mountains, hills, desert, sky  \\\n",
       "0         0    0      0  ...                 0                              0   \n",
       "1         0    0      0  ...                 0                              0   \n",
       "2         0    0      0  ...                 0                              0   \n",
       "3         0    0      0  ...                 0                              0   \n",
       "4         0    0      0  ...                 0                              0   \n",
       "\n",
       "   forest, field, jungle  man-made elements  \\\n",
       "0                      0                  1   \n",
       "1                      0                  0   \n",
       "2                      1                  1   \n",
       "3                      0                  0   \n",
       "4                      0                  0   \n",
       "\n",
       "   transportation (roads, parking, bridges, boats, airports, etc.)  \\\n",
       "0                                                  1                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   cultural or historical building/place (millitary, religious)  \\\n",
       "0                                                  0              \n",
       "1                                                  0              \n",
       "2                                                  0              \n",
       "3                                                  0              \n",
       "4                                                  0              \n",
       "\n",
       "   sports fields, parks, leisure spaces  industrial and construction  \\\n",
       "0                                     0                            0   \n",
       "1                                     0                            0   \n",
       "2                                     0                            0   \n",
       "3                                     0                            0   \n",
       "4                                     0                            0   \n",
       "\n",
       "   houses, cabins, gardens, and farms  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   1   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "\n",
       "   commercial buildings, shops, markets, cities, and towns  \n",
       "0                                                  0        \n",
       "1                                                  0        \n",
       "2                                                  0        \n",
       "3                                                  0        \n",
       "4                                                  0        \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set indexes for join\n",
    "df_img_id = df.set_index(\"image_id\")\n",
    "id_to_sceneInfo_df_img_id = id_to_sceneInfo_df.set_index(\"image_id\")\n",
    "\n",
    "#Join the scene info with coco concept info\n",
    "df_merged = df_img_id.join(id_to_sceneInfo_df_img_id)\n",
    "df_merged.reset_index(inplace=True)\n",
    "df_merged.to_csv(\"DEPICT_coco.csv\",index=False)\n",
    "print('df_merged shape:', df_merged.shape)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've now generated the base dataframe needed to replicate results from DEPICT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og = pd.read_csv(\"DEPICT_coco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load config and setup environment \n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "path_to_config = 'config.yaml'\n",
    "# paths variable holds all paths from the config\n",
    "paths = load_config(path_to_config)\n",
    "#Dictionary: key: idx | value: concept name\n",
    "# idx corresponds here to the index after only selecting columns[3:83] of a given row\n",
    "idx_to_name = {i:name for i,name in enumerate(df_og.columns[3:83])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 25 permuted captions for each of the 15 coco concepts considered in DEPICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permuted_captions(df_in,num_permutations):\n",
    "    df = df_in.copy()\n",
    "    # loop thorugh each concept\n",
    "    for concept in tqdm(paths['coco_concepts']):\n",
    "        #permute each concept 25 times\n",
    "        for n_perm in tqdm(range(num_permutations)):\n",
    "            temp_df = df.copy()\n",
    "            #permute concept column\n",
    "            temp_df[concept] = np.random.permutation(temp_df[concept])\n",
    "\n",
    "            #list to store each new caption\n",
    "            new_captions = []\n",
    "            for i in range(len(temp_df)):\n",
    "                #get concepts an image has\n",
    "                non_zero_concept_idxs = np.nonzero(temp_df.loc[i][3:83])[0]\n",
    "                caption = \"\"\n",
    "                #for all present concepts append to the current image's caption\n",
    "                for idx in non_zero_concept_idxs:\n",
    "                    name = idx_to_name[idx]\n",
    "                    count = temp_df.loc[i,name]\n",
    "                    caption += str(count) + f\" {name}, \"\n",
    "                #remove final comma and space\n",
    "                caption = caption[:-2]\n",
    "                new_captions.append(caption)\n",
    "            #define new column name and add new column to df\n",
    "            new_col_name = f\"permute_{concept}_{n_perm}\"\n",
    "            df[new_col_name] = new_captions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_permuted = generate_permuted_captions(df_og,25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
